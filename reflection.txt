1. What makes a CSV parser “correct”? We're not asking for additional input-output pairs here, but fairly precise, natural-language descriptions. Put another way, what kinds of general properties should your tests be checking about your CSV parser?
- A CSV parser is "correct" if it is able to produce the same length of entries within the underlying data structure as the number of rows in the given CSV. The CSV parser should be able to differentiate between if the provided CSV has headers versus just a collection of rows as data points. The data itself should be consistent throughout so the same number of elements are in each row and align with the number of columns in the header. If a data type is not in-line with what the column is expecting, then the row is invalid and is up to the discretion of the developer to handle the specific row, whether it be throwing out the row completely, throwing the user an error, or leaving it blank but alerting the user.

2. Suppose we gave you a function that randomly produced CSV data on demand. You could then call this class from your testing code. How might you use this source of random data to expand the power of your testing?
- Something that I as a developer can work on is thinking like the end user and coming up with edge cases that would reveal flaws in my work. It's really easy to tunnel vision of the product and creating it so it fits "xyz" use case. This is different from when someone who has no prior attachment to the work can come along and use it in completely different but valid ways. If there was a function that randomly produced CSV data on demand, it would help me create more robust functions that would fit all the cases that a single developer can not think of. Calling this class also helps increase the rate of development because some inconsequential amount of time is spent just thinking about what tests could break your code. Running the function multiple times could also reveal effectiveness in general as coming up with large sets of data is hard if its not randomly produced. Overall, the function would be very vital and expand the power of my testing through the insight it can provide.

3. In what ways did this sprint differ from prior programming assignments you’ve done? Did anything surprise you? Did you encounter any bugs during your work on this sprint? If yes, what were they and how did you fix them? If not, how do you think that you managed to avoid them? 
- This sprint is very different from prior programming assignments I've done. This specific sprint was very cerebral and required a lot more thinking, sitting down and combing through what a user of need, rather than be given a request and filling out skeleton code to achieve the solution. Formally coming up with my own test cases is something I have never done before as well. In my previous assignments, I would be given a test suite and I would know if my work is sufficient if it passed all the test cases. In this sprint, it's up to me to create these test cases to reveal the flaws in the work and there are definitely things I have left out that I can not see right now. I'm surprised how much trouble this caused me to think outside of what I'm given. Outside of technical problems involving documentation specific interactions, like type-casting, a lot of problems came from me not having enough foresight and experience to think like the target audience for the product. This is good for practice and I would love to bolster this skill as the semester goes on. 